{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing Pipeline using Python**"
      ],
      "metadata": {
        "id": "aCPPiOEjJsdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing is a critical step in data science tasks, ensuring that raw data is transformed into a clean, organized, and structured format suitable for analysis. A data preprocessing pipeline streamlines this complex process by automating a series of steps, enabling data professionals to efficiently and consistently preprocess diverse datasets."
      ],
      "metadata": {
        "id": "mM4L0f1RKAjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a Data Preprocessing Pipeline?**"
      ],
      "metadata": {
        "id": "ioi91rhcKV6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing involves transforming and manipulating raw data to improve its quality, consistency, and relevance for analysis. It encompasses several tasks, including handling missing values, standardizing variables, and removing outliers. By performing these preprocessing steps, data professionals ensure that subsequent analysis is based on reliable and accurate data, leading to better insights and predictions.\n",
        "\n",
        "A data preprocessing pipeline is a systematic and automated approach that combines multiple preprocessing steps into a cohesive workflow. It serves as a roadmap for data professionals, guiding them through the transformations and calculations needed to cleanse and prepare data for analysis. The pipeline consists of interconnected steps, each of which is responsible for a specific preprocessing task, such as:\n",
        "\n",
        "imputing missing values\n",
        "scaling numeric features\n",
        "finding and removing outliers\n",
        "or encoding categorical variables\n",
        "By following the predefined sequence of operations, the pipeline ensures consistency, reproducibility, and efficiency in overall preprocessing steps."
      ],
      "metadata": {
        "id": "7tsFiRR1KWtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Data Preprocessing pipeline should be able to handle missing values, standardize numerical features, remove outliers, and ensure easy replication of preprocessing steps on new datasets. Now, here’s how to create a Data Preprocessing pipeline using Python based on the fundamental functions that every pipeline should perform while preprocessing any dataset:"
      ],
      "metadata": {
        "id": "uWaq_-c_Kem2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAMPLE DATA**"
      ],
      "metadata": {
        "id": "Ig0TFErsLCX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "eHD6uw1KK-89"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"data.csv\")"
      ],
      "metadata": {
        "id": "kvNF7UxvLq5_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ORIGINAL DATA:\")\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeJdeBehL8W-",
        "outputId": "e56d3d51-5443-4dda-d55f-0b38180e6237"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL DATA:\n",
            "   NumericFeature1  NumericFeature2 CategoricalFeature\n",
            "0              1.0                7                  A\n",
            "1              2.0                8                  B\n",
            "2              NaN                9                NaN\n",
            "3              4.0               10                  A\n",
            "4              5.0               11                  B\n",
            "5              6.0               50                  C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here’s how you can use this pipeline to perform all the preprocessing steps using Python:"
      ],
      "metadata": {
        "id": "gBqhdKSYMSbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing_pipeline(data):\n",
        "\n",
        "  #Identify numeric and categorical features\n",
        "  numeric_features= data.select_dtypes(include=['float','int']).columns\n",
        "  categorical_features= data.select_dtypes(include=['object']).columns\n",
        "\n",
        "  #Handle missing values in numeric features\n",
        "  data[numeric_features] = data[numeric_features].fillna(data[numeric_features].mean())\n",
        "\n",
        "  #Detect and handle outliers in numeric features using IQR\n",
        "  for feature in numeric_features:\n",
        "        Q1 = data[feature].quantile(0.25)\n",
        "        Q3 = data[feature].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - (1.5 * IQR)\n",
        "        upper_bound = Q3 + (1.5 * IQR)\n",
        "        data[feature] = np.where((data[feature] < lower_bound) | (data[feature] > upper_bound),\n",
        "                                 data[feature].mean(), data[feature])\n",
        "  #Normalize numeric features\n",
        "  scaler = StandardScaler()\n",
        "  scaled_data = scaler.fit_transform(data[numeric_features])\n",
        "  data[numeric_features] = scaler.transform(data[numeric_features])\n",
        "\n",
        "  #Handle missing values in categorical features\n",
        "  data[categorical_features] = data[categorical_features].fillna(data[categorical_features]).mode().iloc[0]\n",
        "\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "R68Vh_OwMTyt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This pipeline is designed to handle various preprocessing tasks on any given dataset. Let’s explore how each step in the pipeline contributes to the overall preprocessing process:\n",
        "\n",
        "The pipeline begins by identifying the numeric and categorical features in the dataset.\n",
        "Next, the pipeline addresses any missing values present in the numeric features. It fills these missing values with the mean value of each respective numeric feature (you can modify this step according to your desired way of filling in missing values of a numerical feature). It ensures that missing data does not hinder subsequent analysis and computations.\n",
        "The pipeline then identifies and handles outliers within the numeric features using the Interquartile Range (IQR) method. Calculating the quartiles and the IQR determines upper and lower boundaries for outliers. Any values outside these boundaries are replaced with the mean value of the respective numeric feature. This step helps prevent the influence of extreme values on subsequent analyses and model building.\n",
        "After handling missing values and outliers, the pipeline normalizes the numeric features. This process ensures that all numeric features contribute equally to subsequent analysis, avoiding biases caused by varying magnitudes.\n",
        "The pipeline proceeds to handle missing values in the categorical features. It fills these missing values with the mode value, representing the most frequently occurring category."
      ],
      "metadata": {
        "id": "IizOAuiJPQb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform data preprocessing\n",
        "cleaned_data = data_preprocessing_pipeline(data)\n",
        "\n",
        "print(\"Preprocessed Data:\")\n",
        "print(cleaned_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--d-v178PRr4",
        "outputId": "221fdc9a-da3f-443d-8174-c2fc5daded4c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Data:\n",
            "   NumericFeature1  NumericFeature2 CategoricalFeature\n",
            "0        -1.535624        -1.099370                  A\n",
            "1        -0.944999        -0.749128                  A\n",
            "2         0.000000        -0.398886                  A\n",
            "3         0.236250        -0.048645                  A\n",
            "4         0.826874         0.301597                  A\n",
            "5         1.417499         1.994431                  A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing involves transforming and manipulating raw data to improve its quality, consistency, and relevance for analysis. A data preprocessing pipeline is a systematic and automated approach that combines multiple preprocessing steps into a cohesive workflow. It serves as a roadmap for data professionals, guiding them through the transformations and calculations needed to cleanse and prepare data for analysis."
      ],
      "metadata": {
        "id": "YYii-ipgPoF3"
      }
    }
  ]
}